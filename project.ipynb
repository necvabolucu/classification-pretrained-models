{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a176bd2",
   "metadata": {},
   "source": [
    "# Sentiment fine-grained classifications-8 project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7483917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we need to install - If it is already installed you can skip this cell\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9299140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-10 22:40:11.388391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-10 22:40:11.388450: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "#pretrained model\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# base model\n",
    "from torchtext.legacy.data import Field,LabelField,BucketIterator,TabularDataset\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#preprocessing and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00c70a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "def read_dataset(file_path):\n",
    "    \"\"\" Read dataset\n",
    "    Input:\n",
    "        file_path - string the path of the dataset\n",
    "    Returns:\n",
    "        train dataframe \n",
    "    \"\"\"\n",
    "    train_data = pd.read_excel(file_path, 'Sheet1')\n",
    "    \n",
    "    ''' Should/Must statement\n",
    "        Should/must statement\n",
    "        should/must statement labels are \n",
    "        converted to Should/Must statement\n",
    "        \n",
    "        personalizing is converted to Personalizing''' \n",
    "    \n",
    "    \n",
    "    train_data.loc[(train_data['label'] == 'should/must statement') | (train_data['label'] == 'Should/must statement')] = 'Should/Must statement' \n",
    "    train_data.loc[train_data['label'] == 'personalizing'] = 'Personalizing' \n",
    "    \n",
    "    #Label encoding \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    train_data[\"label_encoded\"] = le.fit_transform(train_data[\"label\"]) \n",
    "    np.save('classes.npy', le.classes_)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86f0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils function \n",
    "def split_dataset_base(dataframe):\n",
    "    \"\"\" Split dataset into train, val and test\n",
    "    Input:\n",
    "        dataframe - dataframe dataset\n",
    "    Returns:\n",
    "        train_df dataframe train dataframe\n",
    "        val_df dataframe val dataframe\n",
    "        test_df dataframe test dataframe\n",
    "    \"\"\"   \n",
    "    # split train dataset into train, validation and test sets\n",
    "    df, test_df = train_test_split(dataframe,random_state=seed,test_size=0.2, stratify=dataframe[\"label_encoded\"])\n",
    "    \n",
    "    train_df, val_df = train_test_split(df,random_state=seed,test_size=0.2, stratify=df[\"label_encoded\"])\n",
    "    \n",
    "    save_files(train_df, val_df, test_df)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def split_dataset_pretrained(dataframe):\n",
    "    \"\"\" Split dataset into train, val and test\n",
    "    Input:\n",
    "        dataframe - dataframe dataset\n",
    "    Returns:\n",
    "        X_train list train sentences\n",
    "        y_train list label of train dataset\n",
    "        X_val list val sentences\n",
    "        y_val list label of val dataset\n",
    "        X_test list test sentences\n",
    "        y_test list label of test dataset\n",
    "    \"\"\"\n",
    "    X_train, temp_text, y_train, temp_labels = train_test_split(list(dataframe[\"sentences\"].values), list(dataframe[\"label_encoded\"].values), \n",
    "                                                                    random_state=seed, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=list(dataframe[\"label_encoded\"].values))\n",
    "\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=seed, \n",
    "                                                                test_size=0.4, \n",
    "                                                                stratify=temp_labels)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def save_files(out_path, train_df, val_df, test_df):\n",
    "    \"\"\" Save splittted dataset into folder\n",
    "    Input:\n",
    "        out_path string path for saving the files\n",
    "        train_df dataframe train dataframe\n",
    "        val_df dataframe val dataframe\n",
    "        test_df dataframe test dataframe\n",
    "    \"\"\"  \n",
    "    train_df.to_csv(out_path+'train.csv',index=False)\n",
    "    val_df.to_csv(out_path+'val.csv',index=False)\n",
    "    test_df.to_csv(out_path+'test.csv',index=False) \n",
    "   \n",
    "def tokenize(s): \n",
    "    \"\"\" Split text\n",
    "    Input:\n",
    "        s string text to split\n",
    "    Returns:\n",
    "        string splittex text\n",
    "    \"\"\"  \n",
    "    return s.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcf1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Compute metrics for evaluation\n",
    "    p Lists prediction and gold labels for evaluation\n",
    "    Reurns:\n",
    "        eval_scores dictionary evaluation scores\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "\n",
    "    eval_scores = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    return eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89dc9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model and tokenizer based on your pretraine model\n",
    "def bert_model(output_label):\n",
    "    \"\"\" Define bert pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def distilbert_model(output_label):\n",
    "    \"\"\" Define distilbert pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def alberta_model(output_label):\n",
    "    \"\"\" Define alberta pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"albert-base-v2\"\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def gpt2_model(output_label):\n",
    "    \"\"\" Define GPT2 pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "    \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6577a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model based on your pretrained model\n",
    "def test_bert_model(model_path, dataset):\n",
    "    \"\"\" Test with bert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_bert_model_one_sentence(model_path,sentence,classes):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "    \n",
    "\n",
    "def test_distilbert_model(model_path, dataset):\n",
    "    \"\"\" Test with distilbert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_distilbert_model_one_Sentence:\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_alberta_model(model, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_alberta_model_one_Sentence:\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_gpt2_model(model_path, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_gpt2_model_one_Sentence:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f700d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function split dataset into train test and val and convert to torch.data.Dataset\n",
    "def prepare_dataset_pretrained(tokenizer, dataset):\n",
    "    \"\"\" Prepare dataset\n",
    "    Input:\n",
    "        tokenizer - pretrained tokenizer\n",
    "        dataset - dataframe \n",
    "    Returns:\n",
    "        train_dataset - torch.utils.data.Dataset train Dataset\n",
    "        val_dataset - torch.utils.data.Dataset val Dataset\n",
    "        test_dataset - torch.utils.data.Dataset test Dataset\n",
    "        y_test - list gold labels for the test data\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_dataset_pretrained(dataset)\n",
    "    \n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)      \n",
    "    \n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "    test_dataset = Dataset(X_test_tokenized, y_test)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54402a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for pretraine model\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b61bec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # cuda algorithms\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "num_epochs = 500\n",
    "model_name = 'lstm' # model name (bert, alberta, distilbert or gpt2  for pretrained) (lstm, rnn, bilestm for base model)\n",
    "output_path = \"output-bert\" #create a folder to save pretrained model\n",
    "model_path = \"lstm\"\n",
    "embedding_path = \"embeddings/glove.6B.50d.txt\"\n",
    "max_length = 512\n",
    "dataset_base = True # boolean value to split dataset into \n",
    "dataset_path = \"data/\" # path where to save splitted data (it is necessary is dataset_base is True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use 'cuda' if available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c909be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for pretrained models\n",
    "#split into train val and test\n",
    "dataset = read_dataset('L2400.xlsx')\n",
    "\n",
    "num_output = len(set(dataset[\"label_encoded\"])) # number of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a12aff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# select model for pretrained models\n",
    "if model_name == 'bert':\n",
    "    tokenizer, model = bert_model(num_output)\n",
    "elif model_name == 'alberta':\n",
    "    tokenizer, model = alberta_model(num_output)\n",
    "elif model_name == 'distilbert':\n",
    "    tokenizer, model = distilbert_model(num_output)\n",
    "elif model_name == 'gpt2':\n",
    "    tokenizer, model = gpt2_model(num_output)\n",
    "else:\n",
    "    print('model is not defined')\n",
    "    \n",
    "train_dataset, val_dataset, test_dataset, y_test = prepare_dataset_pretrained(tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pretrained model\n",
    "args = TrainingArguments(\n",
    "output_dir = output_path,\n",
    "evaluation_strategy = 'steps',\n",
    "eval_steps = 500,\n",
    "per_device_train_batch_size = batch_size,\n",
    "per_device_eval_batch_size = batch_size,\n",
    "num_train_epochs = num_epochs,\n",
    "seed = seed,\n",
    "load_best_model_at_end = True,)   \n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=val_dataset,\n",
    "compute_metrics=compute_metrics,\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model pretrained models\n",
    "raw_pred, _, _ = trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "y = (raw_pred, y_test)\n",
    "compute_metrics(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only test your data on trained model without training phase - pretrained models\n",
    "model_path = \"path where you stored your model\"\n",
    "test_data_file = \"path of the file\"\n",
    "test_dataset = read_dataset(test_data_file)\n",
    "    \n",
    "if model_name == 'bert':\n",
    "    predictions = test_bert_model(model_path, test_dataset)\n",
    "elif model_name == 'alberta':\n",
    "    predictions = test_alberta_model(model_path, test_dataset)\n",
    "elif model_name == 'distilbert':\n",
    "    predictions = test_distilbert_model(model_path, test_dataset)\n",
    "elif model_name == 'gpt2':\n",
    "    predictions = test_gpt2_model(model_path, test_dataset)\n",
    "else:\n",
    "    print('model is not defined')\n",
    "    \n",
    "y_true = list(dataset[\"label_encoded\"].values)\n",
    "y = (predictions, y_true)\n",
    "compute_metrics(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4249f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/necva/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/necva/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/necva/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/necva/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file output-bert/checkpoint-500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output-bert/checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output-bert/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Labeling']\n"
     ]
    }
   ],
   "source": [
    "#only test on one sentence\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('classes.npy', allow_pickle=True)\n",
    "\n",
    "sentence = \"it is a school\"\n",
    "\n",
    "if model_name == 'bert':\n",
    "    label_sentence = test_bert_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'alberta':\n",
    "    label_sentence = test_alberta_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'distilbert':\n",
    "    label_sentence = test_distilbert_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'gpt2':\n",
    "    label_sentence = test_gpt2_model_one_sentence(model_path, sentence, le.classes_)\n",
    "else:\n",
    "    print('model is not defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a380544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for Base models\n",
    "class Network(torch.nn.Module):\n",
    "    '''\n",
    "    It inherits the functionality of Module class from torch.nn whic includes al the layers, weights, grads setup\n",
    "    and methods to calculate the same. We just need to put in the required layers and describe the flows as\n",
    "    which layers comes after which one\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,in_neuron,embedding_dim=50,hidden_size=256,out_neuron=8,m_type='lstm',drop=0.2,**kwargs):\n",
    "        '''\n",
    "        Constructor of the class which will instantiate the layers while initialisation.\n",
    "        \n",
    "        Input:\n",
    "            in_neuron: input dimensions of the first layer {int}\n",
    "            embedding_dim: number of latent features you want to calculate from the input data {int} default=128\n",
    "            hidden_size: neurons you want to have in your hidden RNN layer {int} default=256\n",
    "            out_neuron: number of outputs you want to have at the end.{int} default=1\n",
    "            model: whether to use 'rnn' or 'lstm' {string} \n",
    "            drop: proportion of values to dropout from the previous values randomly {float 0-1} default=0.53\n",
    "            **kwargs: any torch.nn.RNN or torch.nn.LSTM args given m_type='rnn' or'lstm' {dict}\n",
    "        Returns: \n",
    "            A tensor of shape {batch,out_neuron} as output \n",
    "        '''\n",
    "        super(Network,self).__init__() \n",
    "        self.m_type = m_type\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(in_neuron,embedding_dim) # embedding layer is always the first layer\n",
    "        if self.m_type == \"bilstm\":\n",
    "            self.bilstm = torch.nn.LSTM(embedding_dim,hidden_size,bidirectional=True, **kwargs)\n",
    "        elif self.m_type == 'lstm':\n",
    "        # whether to use the LSTM type model or the RNN type model. It'll use only 1 in forward()\n",
    "            self.lstm = torch.nn.LSTM(embedding_dim,hidden_size,**kwargs)\n",
    "        else:\n",
    "            self.rnn = torch.nn.RNN(embedding_dim,hidden_size,**kwargs) \n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(drop) # drop the values by random which comes from previous layer\n",
    "        if self.m_type == \"bilstm\":\n",
    "            self.dense = torch.nn.Linear(hidden_size*2,out_neuron) # last fully connected layer\n",
    "        else:\n",
    "            self.dense = torch.nn.Linear(hidden_size,out_neuron) # last fully connected layer\n",
    "    \n",
    "    def forward(self,t):\n",
    "        '''\n",
    "        Activate the forward propagation of a batch at a time to transform the input bath of tensors through\n",
    "        the different layers to get an out which then will be compared to original label for computing loss.\n",
    "        Input:\n",
    "            t: tensors in the form of a batch {torch.tensor}\n",
    "        Returns:\n",
    "            output of the network\n",
    "        '''\n",
    "        embedding_t = self.embedding(t)\n",
    "        \n",
    "        drop_emb = self.dropout(embedding_t)\n",
    "        \n",
    "        if self.m_type == \"bilstm\":\n",
    "            out, (hidden_state,_) = self.bilstm(drop_emb)\n",
    "            hidden_state = torch.cat((hidden_state[0,:,:],hidden_state[1,:,:]), dim=1)\n",
    "        elif self.m_type == 'lstm':\n",
    "            out, (hidden_state,_) = self.lstm(drop_emb)\n",
    "        else:\n",
    "            out, hidden_state = self.rnn(drop_emb)\n",
    "            #  shape of rnn_out = (seq_len, batch, num_directions * hidden_size)\n",
    "       \n",
    "        hidden_squeezed = hidden_state.squeeze(0) \n",
    "        \n",
    "        return self.dense(hidden_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfe718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_base(dataset, train):\n",
    "    \"\"\" Prepare dataset\n",
    "    Input:\n",
    "        dataset - dataframe \n",
    "    Returns:\n",
    "        train_dataset - BucketIterator train Dataset\n",
    "        val_dataset - BucketIterator val Dataset\n",
    "        test_dataset - BucketIteratortest Dataset\n",
    "        input_size int input size of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    text_field = Field(tokenize=tokenize)\n",
    "    label_field = LabelField(dtype=torch.float) \n",
    "    # useful for label string to LabelEncoding. Not useful here but doesn't hurt either\n",
    "    \n",
    "    fields = [('sentences',text_field),('label_encoded',label_field)] \n",
    "    # (column name,field object to use on that column) pair for the dictonary\n",
    "    \n",
    "    glove = vocab.Vectors(embedding_path, dataset_path)\n",
    "    if train: #prepare train val and est dataset\n",
    "\n",
    "        if not dataset_base: #Ä±f dataset is not saved\n",
    "            train_df, val_df, test_df = split_dataset_base(dataset)\n",
    "            \n",
    "        train_dataset, val_dataset, test_dataset = TabularDataset.splits(path=dataset_path, train='train.csv',validation='val.csv',test='test.csv', \n",
    "                                                 format='csv',skip_header=True,fields=fields)\n",
    "        \n",
    "        \n",
    "        \n",
    "        text_field.build_vocab(train_dataset,max_size=100000,vectors=glove,unk_init=torch.Tensor.zero_) \n",
    "        label_field.build_vocab(train_dataset) \n",
    "        input_size = len(text_field.vocab)\n",
    "        train_iter, val_iter, test_iter = BucketIterator.splits((train_dataset, val_dataset, test_dataset), batch_sizes=(32,128,128),\n",
    "                                                      sort_key=lambda x: len(x.sentences),\n",
    "                                                      sort_within_batch=False,\n",
    "                                                      device=device) # use the cuda device if available\n",
    "        return train_iter, val_iter, test_iter, input_size\n",
    "    else: #prepare dataset for tes\n",
    "        test_dataset = TabularDataset(path=dataset_path+'test.csv', \n",
    "                                             format='csv',skip_header=True,fields=fields)\n",
    "        \n",
    "        text_field.build_vocab(test_dataset,max_size=100000,vectors=glove,unk_init=torch.Tensor.zero_) \n",
    "        label_field.build_vocab(test_dataset)\n",
    "        \n",
    "        test_iter = BucketIterator(test_dataset, batch_size=32,\n",
    "                                                      sort_key=lambda x: len(x.sentences),\n",
    "                                                      sort=False,\n",
    "                                                      sort_within_batch=False,\n",
    "                                                      device=device) # use the cuda device if available\n",
    "    \n",
    "        return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e97c4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load models for base models\n",
    "def save_checkpoint(save_path, model_name, optimizer, valid_loss, in_neuron):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss,\n",
    "                 'input_size':in_neuron}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model_name):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model = Network(state_dict['input_size'], m_type=model_name) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988794c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function for base models\n",
    "def train_network(network,train_iter,optimizer,loss_fn,epoch_num):\n",
    "    '''\n",
    "    train the network using given parameters\n",
    "    Input:\n",
    "        network: any Neural Network object \n",
    "        train_batch: iterator of training data\n",
    "        optimizer: optimizer for gradients calculation and updation\n",
    "        loss_fn: appropriate loss function\n",
    "        epoch_num = Epoch number so that it can show which epoch number in tqdm Bar\n",
    "    Returns:\n",
    "        a tuple of (average_loss,average_accuracy) of floating values for a single epoch\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    network.train() \n",
    "    \n",
    "    for batch in tqdm(train_iter,f\"Epoch: {epoch_num}\"): \n",
    "        optimizer.zero_grad() \n",
    "        predictions = network(batch.sentences).squeeze(1) \n",
    "        loss = loss_fn(predictions,batch.label_encoded.to(torch.long)) \n",
    "        pred_classes = F.softmax(predictions, dim=1)\n",
    "        pred_classes = torch.argmax(pred_classes, dim=1)\n",
    "        correct_preds = (pred_classes == batch.label_encoded).float()\n",
    "        accuracy = correct_preds.sum()/len(correct_preds)# it'll be a tensor of shape [1,]\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += accuracy.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss/len(train_iter), epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21eb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function for base models\n",
    "def evaluate_network(network,val_test_iter,optimizer,loss_fn):\n",
    "    '''\n",
    "    evaluate the network using given parameters\n",
    "    args:\n",
    "        network: any Neural Network object \n",
    "        val_test_iter: iterator of validation/test data\n",
    "        optimizer: optimizer for gradients calculation and updation\n",
    "        loss_fn: appropriate loss function\n",
    "    out:\n",
    "        a tuple of (average_loss,average_accuracy) of floating values for the incoming dataset\n",
    "    '''\n",
    "    total_loss = 0 \n",
    "    total_acc = 0\n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in val_test_iter:\n",
    "\n",
    "            predictions = network(batch.sentences).squeeze(1)\n",
    "            loss = loss_fn(predictions,batch.label_encoded.to(torch.long))\n",
    "            pred_classes = torch.argmax(predictions, dim=1)\n",
    "            correct_preds = (pred_classes == batch.label_encoded).float()\n",
    "            accuracy = correct_preds.sum()/len(correct_preds)\n",
    "            total_loss += loss.item() \n",
    "            total_acc += accuracy.item()\n",
    "\n",
    "        return total_loss/len(val_test_iter), total_acc/len(val_test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base model\n",
    "train_iter, val_iter, test_iter, in_neuron  = prepare_dataset_base(dataset, True)\n",
    "\n",
    "network = Network(in_neuron, m_type=model_name) \n",
    "if torch.cuda.is_available():\n",
    "    network.cuda() \n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=lr) \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_network(network,train_iter,optimizer,loss_fn,epoch+1)\n",
    "        val_loss,val_acc = evaluate_network(network,val_iter,optimizer,loss_fn)\n",
    "        tqdm.write(f'''End of Epoch: {epoch+1}  |  Train Loss: {train_loss:.3f}  |  Val Loss: {val_loss:.3f}  |  Train Acc: {train_acc*100:.2f}%  |  Val Acc: {val_acc*100:.2f}%''')\n",
    "        \n",
    "test_loss,test_acc = evaluate_network(network,test_iter,optimizer,loss_fn)\n",
    "save_checkpoint(output_path + '/model.pt', network, optimizer, val_loss, in_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model without training\n",
    "dataset = read_dataset('L2400.xlsx')\n",
    "test_data_file = \"path of the file\"\n",
    "#dataset = read_dataset(test_data_file)\n",
    "\n",
    "test_iter  = prepare_dataset_base(dataset, False)\n",
    "\n",
    "\n",
    "model, optimizer = load_checkpoint(model_path + '/model.pt', model_name)\n",
    "\n",
    "\n",
    "\n",
    "#network.load_state_dict(torch.load(model_path), strict=False)\n",
    "test_loss,test_acc = evaluate_network(model,test_iter,optimizer,loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
