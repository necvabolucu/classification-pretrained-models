{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1611e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/necva/.local/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: transformers in /home/necva/.local/lib/python3.8/site-packages (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: filelock in /home/necva/.local/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (2021.9.30)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/necva/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: typing-extensions in /home/necva/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Libraries we need to install - If it is already installed you can skip this cell\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0633c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "# base model\n",
    "from torchtext.legacy.data import Field,LabelField,BucketIterator,TabularDataset\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#preprocessing and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b4a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # cuda algorithms\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "num_epochs = 500\n",
    "model_name = 'lstm' # model name (bert, alberta, distilbert or gpt2  for pretrained) (lstm, rnn, bilestm for base model)\n",
    "output_path = \"output-bert\" #create a folder to save pretrained model\n",
    "model_path = \"lstm\"\n",
    "embedding_path = \"embeddings/glove.6B.50d.txt\"\n",
    "max_length = 512\n",
    "dataset_base = True # boolean value to split dataset into \n",
    "dataset_path = \"data/\" # path where to save splitted data (it is necessary is dataset_base is True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use 'cuda' if available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42fc7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils function \n",
    "def split_dataset_base(dataframe):\n",
    "    \"\"\" Split dataset into train, val and test\n",
    "    Input:\n",
    "        dataframe - dataframe dataset\n",
    "    Returns:\n",
    "        train_df dataframe train dataframe\n",
    "        val_df dataframe val dataframe\n",
    "        test_df dataframe test dataframe\n",
    "    \"\"\"   \n",
    "    # split train dataset into train, validation and test sets\n",
    "    df, test_df = train_test_split(dataframe,random_state=seed,test_size=0.2, stratify=dataframe[\"label_encoded\"])\n",
    "    \n",
    "    train_df, val_df = train_test_split(df,random_state=seed,test_size=0.2, stratify=df[\"label_encoded\"])\n",
    "    \n",
    "    save_files(train_df, val_df, test_df)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_files(out_path, train_df, val_df, test_df):\n",
    "    \"\"\" Save splittted dataset into folder\n",
    "    Input:\n",
    "        out_path string path for saving the files\n",
    "        train_df dataframe train dataframe\n",
    "        val_df dataframe val dataframe\n",
    "        test_df dataframe test dataframe\n",
    "    \"\"\"  \n",
    "    train_df.to_csv(out_path+'train.csv',index=False)\n",
    "    val_df.to_csv(out_path+'val.csv',index=False)\n",
    "    test_df.to_csv(out_path+'test.csv',index=False) \n",
    "   \n",
    "def tokenize(s): \n",
    "    \"\"\" Split text\n",
    "    Input:\n",
    "        s string text to split\n",
    "    Returns:\n",
    "        string splittex text\n",
    "    \"\"\"  \n",
    "    return s.split(' ')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute metrics for evaluation\n",
    "    p Lists prediction and gold labels for evaluation\n",
    "    Reurns:\n",
    "        eval_scores dictionary evaluation scores\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "\n",
    "    eval_scores = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    return eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6857f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for Base models\n",
    "class Network(torch.nn.Module):\n",
    "    '''\n",
    "    It inherits the functionality of Module class from torch.nn whic includes al the layers, weights, grads setup\n",
    "    and methods to calculate the same. We just need to put in the required layers and describe the flows as\n",
    "    which layers comes after which one\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,in_neuron,embedding_dim=50,hidden_size=256,out_neuron=8,m_type='lstm',drop=0.2,**kwargs):\n",
    "        '''\n",
    "        Constructor of the class which will instantiate the layers while initialisation.\n",
    "        \n",
    "        Input:\n",
    "            in_neuron: input dimensions of the first layer {int}\n",
    "            embedding_dim: number of latent features you want to calculate from the input data {int} default=128\n",
    "            hidden_size: neurons you want to have in your hidden RNN layer {int} default=256\n",
    "            out_neuron: number of outputs you want to have at the end.{int} default=1\n",
    "            model: whether to use 'rnn' or 'lstm' {string} \n",
    "            drop: proportion of values to dropout from the previous values randomly {float 0-1} default=0.53\n",
    "            **kwargs: any torch.nn.RNN or torch.nn.LSTM args given m_type='rnn' or'lstm' {dict}\n",
    "        Returns: \n",
    "            A tensor of shape {batch,out_neuron} as output \n",
    "        '''\n",
    "        super(Network,self).__init__() \n",
    "        self.m_type = m_type\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(in_neuron,embedding_dim) # embedding layer is always the first layer\n",
    "        if self.m_type == \"bilstm\":\n",
    "            self.bilstm = torch.nn.LSTM(embedding_dim,hidden_size,bidirectional=True, **kwargs)\n",
    "        elif self.m_type == 'lstm':\n",
    "        # whether to use the LSTM type model or the RNN type model. It'll use only 1 in forward()\n",
    "            self.lstm = torch.nn.LSTM(embedding_dim,hidden_size,**kwargs)\n",
    "        else:\n",
    "            self.rnn = torch.nn.RNN(embedding_dim,hidden_size,**kwargs) \n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(drop) # drop the values by random which comes from previous layer\n",
    "        if self.m_type == \"bilstm\":\n",
    "            self.dense = torch.nn.Linear(hidden_size*2,out_neuron) # last fully connected layer\n",
    "        else:\n",
    "            self.dense = torch.nn.Linear(hidden_size,out_neuron) # last fully connected layer\n",
    "    \n",
    "    def forward(self,t):\n",
    "        '''\n",
    "        Activate the forward propagation of a batch at a time to transform the input bath of tensors through\n",
    "        the different layers to get an out which then will be compared to original label for computing loss.\n",
    "        Input:\n",
    "            t: tensors in the form of a batch {torch.tensor}\n",
    "        Returns:\n",
    "            output of the network\n",
    "        '''\n",
    "        embedding_t = self.embedding(t)\n",
    "        \n",
    "        drop_emb = self.dropout(embedding_t)\n",
    "        \n",
    "        if self.m_type == \"bilstm\":\n",
    "            out, (hidden_state,_) = self.bilstm(drop_emb)\n",
    "            hidden_state = torch.cat((hidden_state[0,:,:],hidden_state[1,:,:]), dim=1)\n",
    "        elif self.m_type == 'lstm':\n",
    "            out, (hidden_state,_) = self.lstm(drop_emb)\n",
    "        else:\n",
    "            out, hidden_state = self.rnn(drop_emb)\n",
    "            #  shape of rnn_out = (seq_len, batch, num_directions * hidden_size)\n",
    "       \n",
    "        hidden_squeezed = hidden_state.squeeze(0) \n",
    "        \n",
    "        return self.dense(hidden_squeezed)\n",
    "    \n",
    "def prepare_dataset_base(dataset, train):\n",
    "    \"\"\" Prepare dataset\n",
    "    Input:\n",
    "        dataset - dataframe \n",
    "    Returns:\n",
    "        train_dataset - BucketIterator train Dataset\n",
    "        val_dataset - BucketIterator val Dataset\n",
    "        test_dataset - BucketIteratortest Dataset\n",
    "        input_size int input size of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    text_field = Field(tokenize=tokenize)\n",
    "    label_field = LabelField(dtype=torch.float) \n",
    "    # useful for label string to LabelEncoding. Not useful here but doesn't hurt either\n",
    "    \n",
    "    fields = [('sentences',text_field),('label_encoded',label_field)] \n",
    "    # (column name,field object to use on that column) pair for the dictonary\n",
    "    \n",
    "    glove = vocab.Vectors(embedding_path, dataset_path)\n",
    "    if train: #prepare train val and est dataset\n",
    "\n",
    "        if not dataset_base: #ıf dataset is not saved\n",
    "            train_df, val_df, test_df = split_dataset_base(dataset)\n",
    "            \n",
    "        train_dataset, val_dataset, test_dataset = TabularDataset.splits(path=dataset_path, train='train.csv',validation='val.csv',test='test.csv', \n",
    "                                                 format='csv',skip_header=True,fields=fields)\n",
    "        \n",
    "        \n",
    "        \n",
    "        text_field.build_vocab(train_dataset,max_size=100000,vectors=glove,unk_init=torch.Tensor.zero_) \n",
    "        label_field.build_vocab(train_dataset) \n",
    "        input_size = len(text_field.vocab)\n",
    "        train_iter, val_iter, test_iter = BucketIterator.splits((train_dataset, val_dataset, test_dataset), batch_sizes=(32,128,128),\n",
    "                                                      sort_key=lambda x: len(x.sentences),\n",
    "                                                      sort_within_batch=False,\n",
    "                                                      device=device) # use the cuda device if available\n",
    "        return train_iter, val_iter, test_iter, input_size\n",
    "    else: #prepare dataset for tes\n",
    "        test_dataset = TabularDataset(path=dataset_path+'test.csv', \n",
    "                                             format='csv',skip_header=True,fields=fields)\n",
    "        \n",
    "        text_field.build_vocab(test_dataset,max_size=100000,vectors=glove,unk_init=torch.Tensor.zero_) \n",
    "        label_field.build_vocab(test_dataset)\n",
    "        \n",
    "        test_iter = BucketIterator(test_dataset, batch_size=32,\n",
    "                                                      sort_key=lambda x: len(x.sentences),\n",
    "                                                      sort=False,\n",
    "                                                      sort_within_batch=False,\n",
    "                                                      device=device) # use the cuda device if available\n",
    "    \n",
    "        return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247e4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load models for base models\n",
    "def save_checkpoint(save_path, model_name, optimizer, valid_loss, in_neuron):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss,\n",
    "                 'input_size':in_neuron}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model_name):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model = Network(state_dict['input_size'], m_type=model_name) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9e8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "def read_dataset(file_path):\n",
    "    \"\"\" Read dataset\n",
    "    Input:\n",
    "        file_path - string the path of the dataset\n",
    "    Returns:\n",
    "        train dataframe \n",
    "    \"\"\"\n",
    "    train_data = pd.read_excel(file_path, 'Sheet1')\n",
    "    \n",
    "    ''' Should/Must statement\n",
    "        Should/must statement\n",
    "        should/must statement labels are \n",
    "        converted to Should/Must statement\n",
    "        \n",
    "        personalizing is converted to Personalizing''' \n",
    "    \n",
    "    \n",
    "    train_data.loc[(train_data['label'] == 'should/must statement') | (train_data['label'] == 'Should/must statement')] = 'Should/Must statement' \n",
    "    train_data.loc[train_data['label'] == 'personalizing'] = 'Personalizing' \n",
    "    \n",
    "    #Label encoding \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    train_data[\"label_encoded\"] = le.fit_transform(train_data[\"label\"]) \n",
    "    np.save('classes.npy', le.classes_)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de0f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for pretrained models\n",
    "#split into train val and test\n",
    "dataset = read_dataset('L2400.xlsx')\n",
    "\n",
    "num_output = len(set(dataset[\"label_encoded\"])) # number of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01fc6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function for base models\n",
    "def train_network(network,train_iter,optimizer,loss_fn,epoch_num):\n",
    "    '''\n",
    "    train the network using given parameters\n",
    "    Input:\n",
    "        network: any Neural Network object \n",
    "        train_batch: iterator of training data\n",
    "        optimizer: optimizer for gradients calculation and updation\n",
    "        loss_fn: appropriate loss function\n",
    "        epoch_num = Epoch number so that it can show which epoch number in tqdm Bar\n",
    "    Returns:\n",
    "        a tuple of (average_loss,average_accuracy) of floating values for a single epoch\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    network.train() \n",
    "    \n",
    "    for batch in tqdm(train_iter,f\"Epoch: {epoch_num}\"): \n",
    "        optimizer.zero_grad() \n",
    "        predictions = network(batch.sentences).squeeze(1) \n",
    "        loss = loss_fn(predictions,batch.label_encoded.to(torch.long)) \n",
    "        pred_classes = F.softmax(predictions, dim=1)\n",
    "        pred_classes = torch.argmax(pred_classes, dim=1)\n",
    "        correct_preds = (pred_classes == batch.label_encoded).float()\n",
    "        accuracy = correct_preds.sum()/len(correct_preds)# it'll be a tensor of shape [1,]\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += accuracy.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss/len(train_iter), epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dd57164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function for base models\n",
    "def evaluate_network(network,val_test_iter,optimizer,loss_fn):\n",
    "    '''\n",
    "    evaluate the network using given parameters\n",
    "    args:\n",
    "        network: any Neural Network object \n",
    "        val_test_iter: iterator of validation/test data\n",
    "        optimizer: optimizer for gradients calculation and updation\n",
    "        loss_fn: appropriate loss function\n",
    "    out:\n",
    "        a tuple of (average_loss,average_accuracy) of floating values for the incoming dataset\n",
    "    '''\n",
    "    total_loss = 0 \n",
    "    total_acc = 0\n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in val_test_iter:\n",
    "\n",
    "            predictions = network(batch.sentences).squeeze(1)\n",
    "            loss = loss_fn(predictions,batch.label_encoded.to(torch.long))\n",
    "            pred_classes = torch.argmax(predictions, dim=1)\n",
    "            correct_preds = (pred_classes == batch.label_encoded).float()\n",
    "            accuracy = correct_preds.sum()/len(correct_preds)\n",
    "            total_loss += loss.item() \n",
    "            total_acc += accuracy.item()\n",
    "\n",
    "        return total_loss/len(val_test_iter), total_acc/len(val_test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "759d87dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|█████████████████████████████████| 48/48 [00:01<00:00, 45.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 1  |  Train Loss: 2.081  |  Val Loss: 2.085  |  Train Acc: 12.37%  |  Val Acc: 13.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 61.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 2  |  Train Loss: 2.079  |  Val Loss: 2.088  |  Train Acc: 11.52%  |  Val Acc: 13.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 63.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 3  |  Train Loss: 2.078  |  Val Loss: 2.087  |  Train Acc: 12.96%  |  Val Acc: 9.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 56.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 4  |  Train Loss: 2.078  |  Val Loss: 2.086  |  Train Acc: 14.52%  |  Val Acc: 13.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 60.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 5  |  Train Loss: 2.074  |  Val Loss: 2.092  |  Train Acc: 14.45%  |  Val Acc: 9.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 58.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 6  |  Train Loss: 2.072  |  Val Loss: 2.094  |  Train Acc: 15.30%  |  Val Acc: 11.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 61.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 7  |  Train Loss: 2.068  |  Val Loss: 2.099  |  Train Acc: 15.36%  |  Val Acc: 7.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 61.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 8  |  Train Loss: 2.061  |  Val Loss: 2.116  |  Train Acc: 15.49%  |  Val Acc: 11.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|█████████████████████████████████| 48/48 [00:00<00:00, 60.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 9  |  Train Loss: 2.036  |  Val Loss: 2.075  |  Train Acc: 18.82%  |  Val Acc: 13.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 10  |  Train Loss: 2.019  |  Val Loss: 2.073  |  Train Acc: 19.14%  |  Val Acc: 12.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 11  |  Train Loss: 1.996  |  Val Loss: 2.047  |  Train Acc: 20.64%  |  Val Acc: 14.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 12  |  Train Loss: 1.968  |  Val Loss: 2.017  |  Train Acc: 21.16%  |  Val Acc: 16.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 13: 100%|████████████████████████████████| 48/48 [00:00<00:00, 57.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 13  |  Train Loss: 1.927  |  Val Loss: 2.021  |  Train Acc: 23.11%  |  Val Acc: 17.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14: 100%|████████████████████████████████| 48/48 [00:00<00:00, 55.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 14  |  Train Loss: 1.884  |  Val Loss: 1.975  |  Train Acc: 24.67%  |  Val Acc: 20.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 15  |  Train Loss: 1.863  |  Val Loss: 1.949  |  Train Acc: 26.30%  |  Val Acc: 20.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16: 100%|████████████████████████████████| 48/48 [00:00<00:00, 57.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 16  |  Train Loss: 1.813  |  Val Loss: 1.937  |  Train Acc: 26.95%  |  Val Acc: 22.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 17  |  Train Loss: 1.778  |  Val Loss: 1.915  |  Train Acc: 30.86%  |  Val Acc: 22.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18: 100%|████████████████████████████████| 48/48 [00:00<00:00, 56.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 18  |  Train Loss: 1.780  |  Val Loss: 1.917  |  Train Acc: 29.49%  |  Val Acc: 21.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19: 100%|████████████████████████████████| 48/48 [00:00<00:00, 62.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 19  |  Train Loss: 1.724  |  Val Loss: 1.912  |  Train Acc: 33.59%  |  Val Acc: 23.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20: 100%|████████████████████████████████| 48/48 [00:00<00:00, 56.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 20  |  Train Loss: 1.721  |  Val Loss: 1.906  |  Train Acc: 33.85%  |  Val Acc: 23.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 21  |  Train Loss: 1.659  |  Val Loss: 1.858  |  Train Acc: 36.46%  |  Val Acc: 26.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22: 100%|████████████████████████████████| 48/48 [00:00<00:00, 58.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 22  |  Train Loss: 1.620  |  Val Loss: 1.859  |  Train Acc: 40.10%  |  Val Acc: 27.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23: 100%|████████████████████████████████| 48/48 [00:00<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 23  |  Train Loss: 1.573  |  Val Loss: 1.772  |  Train Acc: 42.19%  |  Val Acc: 31.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|████████████████████████████████| 48/48 [00:00<00:00, 66.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 24  |  Train Loss: 1.528  |  Val Loss: 1.784  |  Train Acc: 43.82%  |  Val Acc: 34.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 25: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 25  |  Train Loss: 1.504  |  Val Loss: 1.769  |  Train Acc: 45.31%  |  Val Acc: 36.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26: 100%|████████████████████████████████| 48/48 [00:00<00:00, 67.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 26  |  Train Loss: 1.460  |  Val Loss: 1.728  |  Train Acc: 47.53%  |  Val Acc: 38.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 27: 100%|████████████████████████████████| 48/48 [00:00<00:00, 64.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 27  |  Train Loss: 1.446  |  Val Loss: 1.735  |  Train Acc: 46.68%  |  Val Acc: 38.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 28: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 28  |  Train Loss: 1.440  |  Val Loss: 1.715  |  Train Acc: 46.16%  |  Val Acc: 39.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 29: 100%|████████████████████████████████| 48/48 [00:00<00:00, 59.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 29  |  Train Loss: 1.415  |  Val Loss: 1.728  |  Train Acc: 49.48%  |  Val Acc: 39.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 30  |  Train Loss: 1.380  |  Val Loss: 1.707  |  Train Acc: 50.33%  |  Val Acc: 40.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 31  |  Train Loss: 1.356  |  Val Loss: 1.649  |  Train Acc: 51.69%  |  Val Acc: 42.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 32: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 32  |  Train Loss: 1.315  |  Val Loss: 1.643  |  Train Acc: 53.71%  |  Val Acc: 42.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 33: 100%|████████████████████████████████| 48/48 [00:00<00:00, 59.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 33  |  Train Loss: 1.287  |  Val Loss: 1.639  |  Train Acc: 54.10%  |  Val Acc: 43.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 34: 100%|████████████████████████████████| 48/48 [00:00<00:00, 64.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 34  |  Train Loss: 1.265  |  Val Loss: 1.677  |  Train Acc: 55.60%  |  Val Acc: 41.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 35: 100%|████████████████████████████████| 48/48 [00:00<00:00, 64.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 35  |  Train Loss: 1.224  |  Val Loss: 1.629  |  Train Acc: 57.62%  |  Val Acc: 44.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 36: 100%|████████████████████████████████| 48/48 [00:00<00:00, 59.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 36  |  Train Loss: 1.210  |  Val Loss: 1.611  |  Train Acc: 56.77%  |  Val Acc: 44.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 37: 100%|████████████████████████████████| 48/48 [00:00<00:00, 62.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 37  |  Train Loss: 1.198  |  Val Loss: 1.585  |  Train Acc: 58.20%  |  Val Acc: 40.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 38: 100%|████████████████████████████████| 48/48 [00:00<00:00, 51.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 38  |  Train Loss: 1.162  |  Val Loss: 1.573  |  Train Acc: 58.66%  |  Val Acc: 40.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 39: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 39  |  Train Loss: 1.165  |  Val Loss: 1.609  |  Train Acc: 59.70%  |  Val Acc: 42.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40: 100%|████████████████████████████████| 48/48 [00:00<00:00, 62.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 40  |  Train Loss: 1.135  |  Val Loss: 1.623  |  Train Acc: 60.55%  |  Val Acc: 41.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 41: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 41  |  Train Loss: 1.133  |  Val Loss: 1.585  |  Train Acc: 58.72%  |  Val Acc: 44.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 42: 100%|████████████████████████████████| 48/48 [00:00<00:00, 58.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 42  |  Train Loss: 1.078  |  Val Loss: 1.580  |  Train Acc: 63.15%  |  Val Acc: 44.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 43: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 43  |  Train Loss: 1.086  |  Val Loss: 1.562  |  Train Acc: 62.43%  |  Val Acc: 40.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 44: 100%|████████████████████████████████| 48/48 [00:00<00:00, 64.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 44  |  Train Loss: 1.052  |  Val Loss: 1.536  |  Train Acc: 63.09%  |  Val Acc: 41.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 45: 100%|████████████████████████████████| 48/48 [00:00<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 45  |  Train Loss: 1.037  |  Val Loss: 1.553  |  Train Acc: 61.85%  |  Val Acc: 45.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 46: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 46  |  Train Loss: 1.006  |  Val Loss: 1.529  |  Train Acc: 65.89%  |  Val Acc: 51.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 47: 100%|████████████████████████████████| 48/48 [00:00<00:00, 64.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 47  |  Train Loss: 1.027  |  Val Loss: 1.529  |  Train Acc: 63.54%  |  Val Acc: 46.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 48: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 48  |  Train Loss: 1.001  |  Val Loss: 1.540  |  Train Acc: 65.49%  |  Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 49  |  Train Loss: 0.972  |  Val Loss: 1.540  |  Train Acc: 67.51%  |  Val Acc: 49.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 50  |  Train Loss: 0.947  |  Val Loss: 1.540  |  Train Acc: 68.42%  |  Val Acc: 49.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 51: 100%|████████████████████████████████| 48/48 [00:00<00:00, 58.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 51  |  Train Loss: 0.964  |  Val Loss: 1.492  |  Train Acc: 67.58%  |  Val Acc: 49.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 52: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 52  |  Train Loss: 0.936  |  Val Loss: 1.524  |  Train Acc: 67.71%  |  Val Acc: 47.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 53: 100%|████████████████████████████████| 48/48 [00:00<00:00, 59.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 53  |  Train Loss: 0.901  |  Val Loss: 1.516  |  Train Acc: 68.82%  |  Val Acc: 49.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 54: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 54  |  Train Loss: 0.907  |  Val Loss: 1.536  |  Train Acc: 68.75%  |  Val Acc: 49.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 55: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 55  |  Train Loss: 0.908  |  Val Loss: 1.553  |  Train Acc: 69.99%  |  Val Acc: 49.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 56: 100%|████████████████████████████████| 48/48 [00:00<00:00, 51.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 56  |  Train Loss: 0.871  |  Val Loss: 1.531  |  Train Acc: 70.57%  |  Val Acc: 51.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 57: 100%|████████████████████████████████| 48/48 [00:01<00:00, 46.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 57  |  Train Loss: 0.867  |  Val Loss: 1.555  |  Train Acc: 71.74%  |  Val Acc: 49.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 58: 100%|████████████████████████████████| 48/48 [00:01<00:00, 46.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 58  |  Train Loss: 0.848  |  Val Loss: 1.482  |  Train Acc: 71.88%  |  Val Acc: 51.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 59: 100%|████████████████████████████████| 48/48 [00:00<00:00, 66.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 59  |  Train Loss: 0.821  |  Val Loss: 1.506  |  Train Acc: 72.66%  |  Val Acc: 50.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 60: 100%|████████████████████████████████| 48/48 [00:00<00:00, 58.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 60  |  Train Loss: 0.813  |  Val Loss: 1.473  |  Train Acc: 71.74%  |  Val Acc: 52.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 61: 100%|████████████████████████████████| 48/48 [00:00<00:00, 55.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 61  |  Train Loss: 0.821  |  Val Loss: 1.466  |  Train Acc: 73.05%  |  Val Acc: 51.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 62: 100%|████████████████████████████████| 48/48 [00:00<00:00, 54.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 62  |  Train Loss: 0.801  |  Val Loss: 1.469  |  Train Acc: 73.37%  |  Val Acc: 51.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 63: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 63  |  Train Loss: 0.796  |  Val Loss: 1.463  |  Train Acc: 73.37%  |  Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 64: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 64  |  Train Loss: 0.795  |  Val Loss: 1.517  |  Train Acc: 73.83%  |  Val Acc: 51.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 65: 100%|████████████████████████████████| 48/48 [00:00<00:00, 63.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 65  |  Train Loss: 0.781  |  Val Loss: 1.492  |  Train Acc: 74.80%  |  Val Acc: 51.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 66: 100%|████████████████████████████████| 48/48 [00:00<00:00, 62.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 66  |  Train Loss: 0.753  |  Val Loss: 1.432  |  Train Acc: 75.52%  |  Val Acc: 52.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 67: 100%|████████████████████████████████| 48/48 [00:00<00:00, 52.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 67  |  Train Loss: 0.788  |  Val Loss: 1.444  |  Train Acc: 73.83%  |  Val Acc: 53.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 68: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 68  |  Train Loss: 0.708  |  Val Loss: 1.434  |  Train Acc: 76.11%  |  Val Acc: 53.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 69: 100%|████████████████████████████████| 48/48 [00:00<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 69  |  Train Loss: 0.721  |  Val Loss: 1.445  |  Train Acc: 76.04%  |  Val Acc: 53.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 70: 100%|████████████████████████████████| 48/48 [00:00<00:00, 59.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 70  |  Train Loss: 0.746  |  Val Loss: 1.429  |  Train Acc: 75.46%  |  Val Acc: 52.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 71: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 71  |  Train Loss: 0.698  |  Val Loss: 1.420  |  Train Acc: 76.24%  |  Val Acc: 52.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 72: 100%|████████████████████████████████| 48/48 [00:00<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 72  |  Train Loss: 0.705  |  Val Loss: 1.445  |  Train Acc: 76.17%  |  Val Acc: 53.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 73: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 73  |  Train Loss: 0.695  |  Val Loss: 1.442  |  Train Acc: 77.08%  |  Val Acc: 50.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 74: 100%|████████████████████████████████| 48/48 [00:00<00:00, 65.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 74  |  Train Loss: 0.682  |  Val Loss: 1.428  |  Train Acc: 77.73%  |  Val Acc: 54.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 75: 100%|████████████████████████████████| 48/48 [00:00<00:00, 58.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 75  |  Train Loss: 0.661  |  Val Loss: 1.417  |  Train Acc: 78.12%  |  Val Acc: 54.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 76: 100%|████████████████████████████████| 48/48 [00:00<00:00, 60.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch: 76  |  Train Loss: 0.690  |  Val Loss: 1.435  |  Train Acc: 76.69%  |  Val Acc: 51.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 77:  48%|███████████████▎                | 23/48 [00:00<00:00, 59.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4946/1145006990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'''End of Epoch: {epoch+1}  |  Train Loss: {train_loss:.3f}  |  Val Loss: {val_loss:.3f}  |  Train Acc: {train_acc*100:.2f}%  |  Val Acc: {val_acc*100:.2f}%'''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4946/3830046052.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(network, train_iter, optimizer, loss_fn, epoch_num)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcorrect_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# it'll be a tensor of shape [1,]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train base model\n",
    "train_iter, val_iter, test_iter, in_neuron  = prepare_dataset_base(dataset, True)\n",
    "\n",
    "network = Network(in_neuron, m_type=model_name) \n",
    "if torch.cuda.is_available():\n",
    "    network.cuda() \n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=lr) \n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_network(network,train_iter,optimizer,loss_fn,epoch+1)\n",
    "        val_loss,val_acc = evaluate_network(network,val_iter,optimizer,loss_fn)\n",
    "        tqdm.write(f'''End of Epoch: {epoch+1}  |  Train Loss: {train_loss:.3f}  |  Val Loss: {val_loss:.3f}  |  Train Acc: {train_acc*100:.2f}%  |  Val Acc: {val_acc*100:.2f}%''')\n",
    "        \n",
    "test_loss,test_acc = evaluate_network(network,test_iter,optimizer,loss_fn)\n",
    "save_checkpoint(output_path + '/model.pt', network, optimizer, val_loss, in_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b12988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model without training\n",
    "dataset = read_dataset('L2400.xlsx')\n",
    "test_data_file = \"path of the file\"\n",
    "#dataset = read_dataset(test_data_file)\n",
    "\n",
    "test_iter  = prepare_dataset_base(dataset, False)\n",
    "\n",
    "\n",
    "model, optimizer = load_checkpoint(model_path + '/model.pt', model_name)\n",
    "\n",
    "\n",
    "\n",
    "#network.load_state_dict(torch.load(model_path), strict=False)\n",
    "test_loss,test_acc = evaluate_network(model,test_iter,optimizer,loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
