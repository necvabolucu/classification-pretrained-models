{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a176bd2",
   "metadata": {},
   "source": [
    "# Sentiment fine-grained classifications-8 project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7483917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/necva/.local/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: transformers in /home/necva/.local/lib/python3.8/site-packages (4.11.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: sacremoses in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /home/necva/.local/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (2021.9.30)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: six in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/necva/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions in /home/necva/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Libraries we need to install - If it is already installed you can skip this cell\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9299140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 22:30:08.459256: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-20 22:30:08.459289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "#pretrained model\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# base model\n",
    "from torchtext.legacy.data import Field,LabelField,BucketIterator,TabularDataset\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#preprocessing and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c95e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da73d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_dataset():\n",
    "  conn = sqlite3.connect('../Freelancer/Input/reviews_train.db')\n",
    "  cursor = conn.cursor ()  \n",
    "  table_list = [a for a in cursor.execute(\"SELECT * FROM 'reviews' \")]\n",
    "  names = list(map(lambda x: x[0], cursor.description))\n",
    "  #print(names)\n",
    "  cursor. execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "  reviews  = [a[0] for a in cursor.execute(\"SELECT review_title FROM 'reviews'\")]\n",
    "  ratings = [a[0] for a in cursor.execute(\"SELECT rating_diversity FROM 'reviews'\")]\n",
    "\n",
    "  nan_value = \"None\"\n",
    "  df = pd.DataFrame ({'reviews':reviews,\n",
    "                      'ratings': ratings})\n",
    "\n",
    "  df = df.dropna()\n",
    "\n",
    "  return df[\"reviews\"], df[\"ratings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05ac5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, train_ratings = read_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a06765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = train_reviews.values\n",
    "train_ratings = train_ratings.values\n",
    "#train_reviews = [\" \".join(a) for a in train_reviews]\n",
    "train_ratings = [int(a)-1 for a in train_ratings]\n",
    "train_reviews = train_reviews[0:1000] \n",
    "train_ratings = train_ratings[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86f0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_pretrained(dataframe):\n",
    "    \"\"\" Split dataset into train, val and test\n",
    "    Input:\n",
    "        dataframe - dataframe dataset\n",
    "    Returns:\n",
    "        X_train list train sentences\n",
    "        y_train list label of train dataset\n",
    "        X_val list val sentences\n",
    "        y_val list label of val dataset\n",
    "        X_test list test sentences\n",
    "        y_test list label of test dataset\n",
    "    \"\"\"\n",
    "    X_train, temp_text, y_train, temp_labels = train_test_split(list(dataframe[\"sentences\"].values), list(dataframe[\"label_encoded\"].values), \n",
    "                                                                    random_state=seed, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=list(dataframe[\"label_encoded\"].values))\n",
    "\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=seed, \n",
    "                                                                test_size=0.4, \n",
    "                                                                stratify=temp_labels)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcf1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"Compute metrics for evaluation\n",
    "    p Lists prediction and gold labels for evaluation\n",
    "    Reurns:\n",
    "        eval_scores dictionary evaluation scores\n",
    "    \"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "\n",
    "    eval_scores = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    return eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89dc9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model and tokenizer based on your pretraine model\n",
    "def bert_model(output_label):\n",
    "    \"\"\" Define bert pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def distilbert_model(output_label):\n",
    "    \"\"\" Define distilbert pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def alberta_model(output_label):\n",
    "    \"\"\" Define alberta pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"albert-base-v2\"\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def gpt2_model(output_label):\n",
    "    \"\"\" Define GPT2 pretrained tokenizer and model\n",
    "    Input:\n",
    "        output_label - int the number of classes in the dataset\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        model\n",
    "    \"\"\"\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=output_label)\n",
    "    \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6577a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model based on your pretrained model\n",
    "def test_bert_model(model_path, dataset):\n",
    "    \"\"\" Test with bert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=5) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_bert_model_one_sentence(model_path,sentence,classes):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "    \n",
    "\n",
    "def test_distilbert_model(model_path, dataset):\n",
    "    \"\"\" Test with distilbert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_distilbert_model_one_Sentence(model_path,sentence,classes):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_alberta_model(model, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_alberta_model_one_Sentence(model_path,sentence,classes):\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_gpt2_model(model_path, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_gpt2_model_one_Sentence(model_path,sentence,classes):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f700d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function split dataset into train test and val and convert to torch.data.Dataset\n",
    "def prepare_dataset_pretrained(tokenizer, dataset):\n",
    "    \"\"\" Prepare dataset\n",
    "    Input:\n",
    "        tokenizer - pretrained tokenizer\n",
    "        dataset - dataframe \n",
    "    Returns:\n",
    "        train_dataset - torch.utils.data.Dataset train Dataset\n",
    "        val_dataset - torch.utils.data.Dataset val Dataset\n",
    "        test_dataset - torch.utils.data.Dataset test Dataset\n",
    "        y_test - list gold labels for the test data\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_dataset_pretrained(dataset)\n",
    "    \n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)      \n",
    "    \n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "    test_dataset = Dataset(X_test_tokenized, y_test)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54402a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for pretrained model\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b61bec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "#torch.backends.cudnn.deterministic = True  # cuda algorithms\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 500\n",
    "model_name = 'gpt2' # model name (bert, alberta, distilbert or gpt2  for pretrained) (lstm, rnn, bilestm for base model)\n",
    "output_path = \"output-bert\" #create a folder to save pretrained model\n",
    "model_path = \"bert\"\n",
    "embedding_path = \"embeddings/glove.6B.50d.txt\"\n",
    "max_length = 512\n",
    "dataset_base = True # boolean value to split dataset into \n",
    "dataset_path = \"data/\" # path where to save splitted data (it is necessary is dataset_base is True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use 'cuda' if available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4249f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "def read_dataset(file_path):\n",
    "    \"\"\" Read dataset\n",
    "    Input:\n",
    "        file_path - string the path of the dataset\n",
    "    Returns:\n",
    "        train dataframe \n",
    "    \"\"\"\n",
    "    train_data = pd.read_excel(file_path, 'Sheet1')\n",
    "    \n",
    "    ''' Should/Must statement\n",
    "        Should/must statement\n",
    "        should/must statement labels are \n",
    "        converted to Should/Must statement\n",
    "        \n",
    "        personalizing is converted to Personalizing''' \n",
    "    \n",
    "    \n",
    "    train_data.loc[(train_data['label'] == 'should/must statement') | (train_data['label'] == 'Should/must statement')] = 'Should/Must statement' \n",
    "    train_data.loc[train_data['label'] == 'personalizing'] = 'Personalizing' \n",
    "    \n",
    "    #Label encoding \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    train_data[\"label_encoded\"] = le.fit_transform(train_data[\"label\"]) \n",
    "    np.save('classes.npy', le.classes_)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c909be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for pretrained models\n",
    "#split into train val and test\n",
    "dataset = read_dataset('L2400.xlsx')\n",
    "\n",
    "num_output = len(set(dataset[\"label_encoded\"])) # number of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d77537ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "#le = LabelEncoder()\n",
    "#train_ratings_encoded = le.fit_transform(train_ratings) \n",
    "#np.save('classes.npy', le.classes_)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_reviews, train_ratings, \n",
    "                                                                random_state=seed, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12aff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "num_output = len(set(train_ratings)) \n",
    "print(num_output)\n",
    "# number of classes in the dataset\n",
    "# select model for pretrained models\n",
    "if model_name == 'bert':\n",
    "    tokenizer, model = bert_model(num_output)\n",
    "elif model_name == 'alberta':\n",
    "    tokenizer, model = alberta_model(num_output)\n",
    "elif model_name == 'distilbert':\n",
    "    tokenizer, model = distilbert_model(num_output)\n",
    "elif model_name == 'gpt2':\n",
    "    tokenizer, model = gpt2_model(num_output)\n",
    "else:\n",
    "    print('model is not defined')\n",
    "    \n",
    "#train_dataset, val_dataset, test_dataset, y_test = prepare_dataset_pretrained(tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521f5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_reviews, train_ratings, \n",
    "                                                                random_state=seed, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=train_ratings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbbc23af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9603cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4067/1721079373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_val_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m                 )\n\u001b[1;32m   2402\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2404\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   2580\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2298\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2299\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(list(X_train), padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(list(X_val), padding=True, truncation=True, max_length=512) \n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2cd6d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/necva/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 500\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2021-10-20 22:31:48.702174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-20 22:31:48.702193: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nbolucu/huggingface/runs/3jcimrpf\" target=\"_blank\">output-bert</a></strong> to <a href=\"https://wandb.ai/nbolucu/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2000/12500 1:40:01 < 8:45:39, 0.33 it/s, Epoch 80/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>2.907294</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>3.370244</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>3.781756</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>3.776557</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output-bert/checkpoint-500\n",
      "Configuration saved in output-bert/checkpoint-500/config.json\n",
      "Model weights saved in output-bert/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output-bert/checkpoint-1000\n",
      "Configuration saved in output-bert/checkpoint-1000/config.json\n",
      "Model weights saved in output-bert/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output-bert/checkpoint-1500\n",
      "Configuration saved in output-bert/checkpoint-1500/config.json\n",
      "Model weights saved in output-bert/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to output-bert/checkpoint-2000\n",
      "Configuration saved in output-bert/checkpoint-2000/config.json\n",
      "Model weights saved in output-bert/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from output-bert/checkpoint-500 (score: 2.907294273376465).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.28516904449462893, metrics={'train_runtime': 6013.0716, 'train_samples_per_second': 66.522, 'train_steps_per_second': 2.079, 'total_flos': 690685123968000.0, 'train_loss': 0.28516904449462893, 'epoch': 80.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train pretrained model\n",
    "args = TrainingArguments(\n",
    "output_dir = output_path,\n",
    "evaluation_strategy = 'steps',\n",
    "eval_steps = 500,\n",
    "per_device_train_batch_size = batch_size,\n",
    "per_device_eval_batch_size = batch_size,\n",
    "num_train_epochs = num_epochs,\n",
    "seed = seed,\n",
    "load_best_model_at_end = True,)   \n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=val_dataset,\n",
    "compute_metrics=compute_metrics,\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f7df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.395, 'precision': 0.395, 'recall': 0.395, 'f1': 0.395}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model pretrained models\n",
    "raw_pred, _, _ = trainer.predict(val_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "y = (raw_pred, y_val)\n",
    "compute_metrics(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only test your data on trained model without training phase - pretrained models\n",
    "model_path = \"path where you stored your model\"\n",
    "test_data_file = \"path of the file\"\n",
    "test_dataset = read_dataset(test_data_file)\n",
    "    \n",
    "if model_name == 'bert':\n",
    "    predictions = test_bert_model(model_path, test_dataset)\n",
    "elif model_name == 'alberta':\n",
    "    predictions = test_alberta_model(model_path, test_dataset)\n",
    "elif model_name == 'distilbert':\n",
    "    predictions = test_distilbert_model(model_path, test_dataset)\n",
    "elif model_name == 'gpt2':\n",
    "    predictions = test_gpt2_model(model_path, test_dataset)\n",
    "else:\n",
    "    print('model is not defined')\n",
    "    \n",
    "y_true = list(dataset[\"label_encoded\"].values)\n",
    "y = (predictions, y_true)\n",
    "compute_metrics(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
