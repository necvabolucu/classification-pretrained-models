{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0837e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/necva/.local/lib/python3.8/site-packages (0.1.96)\n",
      "Requirement already satisfied: transformers in /home/necva/.local/lib/python3.8/site-packages (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (2021.9.30)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in /home/necva/.local/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/necva/.local/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /home/necva/.local/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /home/necva/.local/lib/python3.8/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/necva/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /home/necva/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Libraries we need to install - If it is already installed you can skip this cell\n",
    "!pip install sentencepiece\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543ae9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 22:38:55.829091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-11 22:38:55.829165: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "#pretrained model\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# base model\n",
    "from torchtext.legacy.data import Field,LabelField,BucketIterator,TabularDataset\n",
    "from torchtext import vocab\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#preprocessing and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a0c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True  # cuda algorithms\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "max_length = 512\n",
    "model_name = 'bert' # model name (bert, alberta, distilbert or gpt2  for pretrained) (lstm, rnn, bilestm for base model)\n",
    "model_path = \"output-bert/checkpoint-500\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use 'cuda' if available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c13b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model based on your pretrained model\n",
    "def test_bert_model(model_path, dataset):\n",
    "    \"\"\" Test with bert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_bert_model_one_sentence(model_path,sentence,classes):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "    \n",
    "\n",
    "def test_distilbert_model(model_path, dataset):\n",
    "    \"\"\" Test with distilbert pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_distilbert_model_one_Sentence(model_path, dataset):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_alberta_model(model, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_alberta_model_one_Sentence(model_path, dataset):\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def test_gpt2_model(model_path, dataset):\n",
    "    \"\"\" Test with alberta pretrained model\n",
    "    Input:\n",
    "        model_path - path of saved pretrained model\n",
    "    Returns:\n",
    "        raw_pred list predictions of test dataset\n",
    "    \"\"\"\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    raw_pred, _, _ = test_trainer.predict(dataset) \n",
    "    \n",
    "    return raw_pred\n",
    "\n",
    "def test_gpt2_model_one_Sentence(model_path, dataset):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_path, num_labels=8) \n",
    "    test_trainer = Trainer(model)\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    pred = np.argmax(probs.detach().numpy(), axis=1)\n",
    "    \n",
    "    return classes[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b4db6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/necva/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/necva/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/necva/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/necva/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file output-bert/checkpoint-500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output-bert/checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output-bert/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#only test on one sentence\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load('classes.npy', allow_pickle=True)\n",
    "\n",
    "sentence = \"it is a school\"\n",
    "\n",
    "if model_name == 'bert':\n",
    "    label_sentence = test_bert_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'alberta':\n",
    "    label_sentence = test_alberta_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'distilbert':\n",
    "    label_sentence = test_distilbert_model_one_sentence(model_path, sentence, le.classes_)\n",
    "elif model_name == 'gpt2':\n",
    "    label_sentence = test_gpt2_model_one_sentence(model_path, sentence, le.classes_)\n",
    "else:\n",
    "    print('model is not defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02946099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Labeling'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_sentence[0] # predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bf4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "def read_dataset(file_path):\n",
    "    \"\"\" Read dataset\n",
    "    Input:\n",
    "        file_path - string the path of the dataset\n",
    "    Returns:\n",
    "        train dataframe \n",
    "    \"\"\"\n",
    "    train_data = pd.read_excel(file_path, 'Sheet1')\n",
    "    \n",
    "    ''' Should/Must statement\n",
    "        Should/must statement\n",
    "        should/must statement labels are \n",
    "        converted to Should/Must statement\n",
    "        \n",
    "        personalizing is converted to Personalizing''' \n",
    "    \n",
    "    \n",
    "    train_data.loc[(train_data['label'] == 'should/must statement') | (train_data['label'] == 'Should/must statement')] = 'Should/Must statement' \n",
    "    train_data.loc[train_data['label'] == 'personalizing'] = 'Personalizing' \n",
    "    \n",
    "    #Label encoding \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    train_data[\"label_encoded\"] = le.fit_transform(train_data[\"label\"]) \n",
    "    np.save('classes.npy', le.classes_)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only test your data on trained model without training phase - pretrained models\n",
    "model_path = \"path where you stored your model\"\n",
    "test_data_file = \"path of the file\"\n",
    "test_dataset = read_dataset(test_data_file)\n",
    "    \n",
    "if model_name == 'bert':\n",
    "    predictions = test_bert_model(model_path, test_dataset)\n",
    "elif model_name == 'alberta':\n",
    "    predictions = test_alberta_model(model_path, test_dataset)\n",
    "elif model_name == 'distilbert':\n",
    "    predictions = test_distilbert_model(model_path, test_dataset)\n",
    "elif model_name == 'gpt2':\n",
    "    predictions = test_gpt2_model(model_path, test_dataset)\n",
    "else:\n",
    "    print('model is not defined')\n",
    "    \n",
    "y_true = list(dataset[\"label_encoded\"].values)\n",
    "y = (predictions, y_true)\n",
    "compute_metrics(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
